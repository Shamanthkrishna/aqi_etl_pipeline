{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86be5569-d607-4f60-b9be-f36295a5decf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_base = \"/Volumes/aqi_etl_pipeline/bronze_aqi/bronze_vol_aqi/aqi/\"\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# STEP 2 — Find latest YEAR\n",
    "# ==========================================================\n",
    "years = [item.name.replace(\"/\", \"\") for item in dbutils.fs.ls(bronze_base)]\n",
    "years_int = [int(y) for y in years if y.isdigit()]\n",
    "latest_year = str(max(years_int))\n",
    "\n",
    "print(\"Latest Year:\", latest_year)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# STEP 3 — Find latest MONTH\n",
    "# ==========================================================\n",
    "months = [item.name.replace(\"/\", \"\") for item in dbutils.fs.ls(f\"{bronze_base}/{latest_year}\")]\n",
    "months_int = [int(m) for m in months if m.isdigit()]\n",
    "latest_month = str(max(months_int)).zfill(2)\n",
    "\n",
    "print(\"Latest Month:\", latest_month)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# STEP 4 — Find latest DAY\n",
    "# ==========================================================\n",
    "days = [item.name.replace(\"/\", \"\") for item in dbutils.fs.ls(f\"{bronze_base}/{latest_year}/{latest_month}\")]\n",
    "days_int = [int(d) for d in days if d.isdigit()]\n",
    "latest_day = str(max(days_int)).zfill(2)\n",
    "\n",
    "print(\"Latest Day:\", latest_day)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# STEP 5 — Find latest HOUR\n",
    "# ==========================================================\n",
    "hours = [item.name.replace(\"/\", \"\") for item in dbutils.fs.ls(f\"{bronze_base}/{latest_year}/{latest_month}/{latest_day}\")]\n",
    "hours_int = [int(h) for h in hours if h.isdigit()]\n",
    "latest_hour = str(max(hours_int)).zfill(2)\n",
    "\n",
    "print(\"Latest Hour:\", latest_hour)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# STEP 6 — Find latest FILE inside that hour folder\n",
    "# ==========================================================\n",
    "files = dbutils.fs.ls(f\"{bronze_base}/{latest_year}/{latest_month}/{latest_day}/{latest_hour}\")\n",
    "\n",
    "# Extract epoch from file names: weather_<epoch>.json\n",
    "file_epochs = []\n",
    "file_map = {}  # epoch → full path\n",
    "\n",
    "for f in files:\n",
    "    name = f.name  # example: weather_1763967555.json\n",
    "    if name.startswith(\"weather_\") and name.endswith(\".json\"):\n",
    "        epoch = int(name.replace(\"weather_\", \"\").replace(\".json\", \"\"))\n",
    "        file_epochs.append(epoch)\n",
    "        file_map[epoch] = f.path\n",
    "\n",
    "latest_epoch = max(file_epochs)\n",
    "latest_file_path = file_map[latest_epoch]\n",
    "\n",
    "print(\"Latest File:\", latest_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38438a3b-f296-4461-9c93-c97be1eed122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.json(latest_file_path)\n",
    "\n",
    "print(\"Bronze schema:\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce50d5c8-465b-4ee1-bb0c-7829a106cd22",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"coord\":264},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764907311497}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_flat = df_bronze.select(\n",
    "    F.col(\"data.aqi\").alias(\"AQI\"),\n",
    "    F.col(\"data.city.name\").alias(\"City\"),\n",
    "    F.col(\"data.city.geo\").getItem(0).alias(\"latitude\"),\n",
    "    F.col(\"data.city.geo\").getItem(1).alias(\"longitude\"),\n",
    "    F.col(\"data.time.s\").alias(\"timestamp\"),\n",
    "    F.col(\"ingested_at\").alias(\"ingested_at\")\n",
    ")\n",
    "\n",
    "display(df_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fdcdd3-153d-423d-91fc-6c005184bfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_silver = (\n",
    "    df_flat\n",
    "    \n",
    "    # Convert \"timestamp\" from string to IST timestamp\n",
    "    .withColumn(\"aqi_timestamp_ist\", \n",
    "        F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "    )\n",
    "    \n",
    "    # Convert IST → UTC (IST is +5:30 ahead)\n",
    "    .withColumn(\"aqi_timestamp_utc\", \n",
    "        F.expr(\"aqi_timestamp_ist - INTERVAL 5 HOURS 30 MINUTES\")\n",
    "    )\n",
    "    \n",
    "    # Convert ingested_at (UTC string) → Timestamp\n",
    "    .withColumn(\"ingested_at_utc\", \n",
    "        F.to_timestamp(\"ingested_at\")\n",
    "    )\n",
    "    \n",
    "    # Create IST version for ingestion time\n",
    "    .withColumn(\"ingested_at_ist\", \n",
    "        F.expr(\"ingested_at_utc + INTERVAL 5 HOURS 30 MINUTES\")\n",
    "    )\n",
    "    \n",
    "    # Partition-friendly columns\n",
    "    .withColumn(\"aqi_date\", F.to_date(\"aqi_timestamp_ist\"))\n",
    "    .withColumn(\"aqi_hour\", F.hour(\"aqi_timestamp_ist\"))\n",
    ")\n",
    "\n",
    "display(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7d4dd1-14c2-4789-ba30-2f1a537d8087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.printSchema()\n",
    "df_silver.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb6103d-b7b9-4ccb-9215-47c269832818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "silver_table = \"aqi_etl_pipeline.silver.silver_aqi\"\n",
    "\n",
    "# Check table existence safely\n",
    "try:\n",
    "    spark.table(silver_table)\n",
    "    table_exists = True\n",
    "except AnalysisException:\n",
    "    table_exists = False\n",
    "\n",
    "if not table_exists:\n",
    "    (df_silver.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "    print(f\"✔ Created Silver table: {silver_table}\")\n",
    "\n",
    "else:\n",
    "    (df_silver.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "    print(f\"✔ Appended new AQI data to: {silver_table}\")\n",
    "\n",
    "print(\"Done! Silver layer updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6138399557358187,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver_aqi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
