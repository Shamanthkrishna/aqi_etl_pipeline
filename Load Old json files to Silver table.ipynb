{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca916c8-0169-4a5a-88a1-d2f6bdd5f4f4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765128965491}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col, from_json, schema_of_json\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 1: Load ALL JSON files recursively + schema inference enabled\n",
    "# --------------------------------------------------------------------\n",
    "bronze_base = \"/Volumes/aqi_etl_pipeline/bronze_aqi/bronze_vol_aqi/aqi/\"\n",
    "\n",
    "df_bronze_all = (\n",
    "    spark.read\n",
    "        .format(\"json\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .load(bronze_base)\n",
    ")\n",
    "\n",
    "print(\"Bronze total rows:\", df_bronze_all.count())\n",
    "df_bronze_all.printSchema()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 2: Force correct schema for \"data\" if it's a string\n",
    "# --------------------------------------------------------------------\n",
    "# Detect a valid JSON sample\n",
    "sample_data = (\n",
    "    df_bronze_all\n",
    "    .filter(col(\"data\").isNotNull() & (col(\"data\") != \"\"))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "if sample_data.count() > 0:\n",
    "    raw_json = sample_data.first()[\"data\"]\n",
    "    expected_schema = schema_of_json(raw_json)\n",
    "\n",
    "    # Convert data column from string ‚Üí struct\n",
    "    df_bronze_fixed = df_bronze_all.withColumn(\n",
    "        \"data\", from_json(col(\"data\"), expected_schema)\n",
    "    )\n",
    "else:\n",
    "    df_bronze_fixed = df_bronze_all\n",
    "\n",
    "df_bronze_fixed.printSchema()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 3: Filter only valid AQI rows\n",
    "# --------------------------------------------------------------------\n",
    "df_bronze_valid = df_bronze_fixed.filter(col(\"data.aqi\").isNotNull())\n",
    "print(\"Valid AQI rows:\", df_bronze_valid.count())\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 4: Flatten the structure\n",
    "# --------------------------------------------------------------------\n",
    "df_flat_all = df_bronze_valid.select(\n",
    "    col(\"data.aqi\").alias(\"AQI\"),\n",
    "    col(\"data.city.name\").alias(\"City\"),\n",
    "    col(\"data.city.geo\").getItem(1).alias(\"Latitude\"),\n",
    "    col(\"data.city.geo\").getItem(0).alias(\"Longitude\"),\n",
    "    col(\"data.time.s\").alias(\"timestamp\"),\n",
    "    col(\"ingested_at\")\n",
    ")\n",
    "\n",
    "# Clean timestamp format\n",
    "df_flat_all = df_flat_all.filter(col(\"timestamp\").isNotNull())\n",
    "print(\"Flat rows:\", df_flat_all.count())\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 5: Convert timestamps (IST ‚Üê‚Üí UTC)\n",
    "# --------------------------------------------------------------------\n",
    "df_silver_all = (\n",
    "    df_flat_all\n",
    "    .withColumn(\"aqi_timestamp_ist\", \n",
    "        F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "    )\n",
    "    .withColumn(\"aqi_timestamp_utc\", \n",
    "        F.expr(\"aqi_timestamp_ist - INTERVAL 5 HOURS 30 MINUTES\")\n",
    "    )\n",
    "    .withColumn(\"ingested_at_utc\", \n",
    "        F.to_timestamp(\"ingested_at\")\n",
    "    )\n",
    "    .withColumn(\"ingested_at_ist\", \n",
    "        F.expr(\"ingested_at_utc + INTERVAL 5 HOURS 30 MINUTES\")\n",
    "    )\n",
    "    .withColumn(\"aqi_date\", F.to_date(\"aqi_timestamp_ist\"))\n",
    "    .withColumn(\"aqi_hour\", F.hour(\"aqi_timestamp_ist\"))\n",
    ")\n",
    "\n",
    "display(df_silver_all)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 6: Write to Silver Delta Table (Append Only)\n",
    "# --------------------------------------------------------------------\n",
    "silver_table = \"aqi_etl_pipeline.silver.silver_aqi\"\n",
    "\n",
    "# Check table existence safely\n",
    "try:\n",
    "    spark.table(silver_table)\n",
    "    table_exists = True\n",
    "except AnalysisException:\n",
    "    table_exists = False\n",
    "\n",
    "if not table_exists:\n",
    "    (df_silver_all.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "    print(f\"‚úî Silver table created: {silver_table}\")\n",
    "\n",
    "else:\n",
    "    (df_silver_all.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "    print(f\"‚úî Historical AQI appended to: {silver_table}\")\n",
    "\n",
    "print(\"üéØ Backfill completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Old json files to Silver table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
